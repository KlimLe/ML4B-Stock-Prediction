{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlimLe/ML4B-Stock-Prediction/blob/main/AdvancedModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy spacy yfinance scikit-learn transformers tensorflow nltk ta textblob"
      ],
      "metadata": {
        "id": "Dgy6NmdG3sIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff8iy1sJ3NfN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from ta import add_all_ta_features\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Load new financial news dataset\n",
        "news_data = pd.read_csv('/content/first_200_rows_dataset.csv')  # Replace with your dataset path\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "news_data.rename(columns={'News Article': 'News_Article', 'Date': 'Date'}, inplace=True)\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize Spacy model and NLTK components\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# List of companies to focus on\n",
        "companies_to_focus = {\n",
        "    'AMZN': 'Amazon',\n",
        "    'GOOGL': 'Google',\n",
        "    'AAPL': 'Apple'\n",
        "}\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "# Preprocess news articles\n",
        "news_data['Processed_Article'] = news_data['News_Article'].apply(preprocess_text)\n",
        "\n",
        "# Perform Sentiment Analysis\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "news_data[\"Sentiment\"] = news_data[\"Processed_Article\"].apply(get_sentiment)\n",
        "\n",
        "# Initialize BERT tokenizer and model (You can also use RoBERTa or other advanced models)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "def get_bert_embeddings(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding\n",
        "\n",
        "# Calculate BERT embeddings for all news\n",
        "news_data[\"BERT_Embedding\"] = news_data[\"Processed_Article\"].apply(lambda x: get_bert_embeddings([x], tokenizer, bert_model)[0])\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(news_data['Processed_Article'])\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "news_data['TFIDF'] = list(tfidf_array)\n",
        "\n",
        "# Perform Topic Modeling using LDA\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda_matrix = lda.fit_transform(tfidf_matrix)\n",
        "news_data['Topic'] = np.argmax(lda_matrix, axis=1)\n",
        "\n",
        "# Perform NER\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.label_ for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "news_data['Entities'] = news_data['Processed_Article'].apply(extract_entities)\n",
        "\n",
        "# Convert entities to fixed-size vector\n",
        "def entities_to_vector(entities, entity_labels):\n",
        "    counter = Counter(entities)\n",
        "    vector = np.zeros(len(entity_labels))\n",
        "    for i, label in enumerate(entity_labels):\n",
        "        vector[i] = counter[label]\n",
        "    return vector\n",
        "\n",
        "# Define the set of possible entity labels\n",
        "entity_labels = list(nlp.get_pipe('ner').labels)\n",
        "\n",
        "news_data['Entities_Vector'] = news_data['Entities'].apply(lambda x: entities_to_vector(x, entity_labels))\n",
        "\n",
        "# Function to fetch stock prices and fundamental data for each company\n",
        "def fetch_stock_prices(ticker, start_date, end_date):\n",
        "    try:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if stock_data.shape[0] > 14:  # Ensure there are at least 15 rows of data\n",
        "            stock_data = add_all_ta_features(stock_data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")\n",
        "            # Handle missing technical indicators\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            stock_data.iloc[:, :] = imputer.fit_transform(stock_data)\n",
        "        else:\n",
        "            print(f\"Not enough data for {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Filter out rows with missing stock prices\n",
        "        stock_data.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "        # Reset index to get the date column back after filtering\n",
        "        stock_data.reset_index(inplace=True)\n",
        "\n",
        "        return stock_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def fetch_fundamental_data(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    fundamentals = stock.info\n",
        "    return {\n",
        "        \"PE_Ratio\": fundamentals.get(\"trailingPE\", np.nan),\n",
        "        \"EPS\": fundamentals.get(\"trailingEps\", np.nan),\n",
        "        \"Revenue\": fundamentals.get(\"totalRevenue\", np.nan),\n",
        "        \"Market_Cap\": fundamentals.get(\"marketCap\", np.nan)\n",
        "    }\n",
        "\n",
        "# Correct date format and optionally extend the date range\n",
        "from_date = \"2021-01-01\"\n",
        "to_date = \"2021-12-31\"  # Extended date range\n",
        "\n",
        "# Define look-back window\n",
        "look_back = 5\n",
        "\n",
        "# Function to prepare data for each company\n",
        "def prepare_company_data(ticker, company, from_date, to_date):\n",
        "    print(f\"Fetching data for {company} ({ticker})\")\n",
        "    stock_data = fetch_stock_prices(ticker, from_date, to_date)\n",
        "    if stock_data.empty:\n",
        "        print(f\"No stock data found for {company} ({ticker})\")\n",
        "        return None\n",
        "    fundamental_data = fetch_fundamental_data(ticker)\n",
        "\n",
        "    # Filter news for the company or its ticker symbol\n",
        "    company_news = news_data[news_data['News_Article'].str.contains(company, case=False) | news_data['News_Article'].str.contains(ticker, case=False)]\n",
        "\n",
        "    # Aggregate all news by day\n",
        "    all_news_agg = news_data.groupby('Date').agg({\n",
        "        'BERT_Embedding': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "        'Sentiment': 'mean',\n",
        "        'Topic': lambda x: np.mean(x),\n",
        "        'TFIDF': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "        'Entities_Vector': lambda x: np.mean(np.vstack(x), axis=0)\n",
        "    }).reset_index()\n",
        "\n",
        "    # Handle missing dates for all news\n",
        "    all_dates = pd.date_range(start=from_date, end=to_date, freq='D')\n",
        "    all_news_agg = all_news_agg.set_index('Date').reindex(all_dates).reset_index()\n",
        "    all_news_agg.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "    # Insert neutral values for missing dates\n",
        "    all_news_agg['BERT_Embedding'] = all_news_agg['BERT_Embedding'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(bert_model.config.hidden_size))\n",
        "    all_news_agg['Sentiment'] = all_news_agg['Sentiment'].fillna(0.0)\n",
        "    all_news_agg['Topic'] = all_news_agg['Topic'].fillna(-1)\n",
        "    all_news_agg['TFIDF'] = all_news_agg['TFIDF'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(1000))\n",
        "    all_news_agg['Entities_Vector'] = all_news_agg['Entities_Vector'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(len(entity_labels)))\n",
        "\n",
        "    # Aggregate company-specific news by day\n",
        "    if not company_news.empty:\n",
        "        company_news_agg = company_news.groupby('Date').agg({\n",
        "            'BERT_Embedding': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "            'Sentiment': 'mean',\n",
        "            'Topic': lambda x: np.mean(x),\n",
        "            'TFIDF': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "            'Entities_Vector': lambda x: np.mean(np.vstack(x), axis=0)\n",
        "        }).reset_index()\n",
        "\n",
        "        # Handle missing dates for company-specific news\n",
        "        company_news_agg = company_news_agg.set_index('Date').reindex(all_dates).reset_index()\n",
        "        company_news_agg.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "        # Insert neutral values for missing dates\n",
        "        company_news_agg['BERT_Embedding'] = company_news_agg['BERT_Embedding'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(bert_model.config.hidden_size))\n",
        "        company_news_agg['Sentiment'] = company_news_agg['Sentiment'].fillna(0.0)\n",
        "        company_news_agg['Topic'] = company_news_agg['Topic'].fillna(-1)\n",
        "        company_news_agg['TFIDF'] = company_news_agg['TFIDF'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(1000))\n",
        "        company_news_agg['Entities_Vector'] = company_news_agg['Entities_Vector'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(len(entity_labels)))\n",
        "    else:\n",
        "        # Create empty DataFrame with the same structure\n",
        "        company_news_agg = pd.DataFrame({\n",
        "            'Date': all_dates,\n",
        "            'BERT_Embedding': [np.zeros(bert_model.config.hidden_size)] * len(all_dates),\n",
        "            'Sentiment': [0.0] * len(all_dates),\n",
        "            'Topic': [-1] * len(all_dates),\n",
        "            'TFIDF': [np.zeros(1000)] * len(all_dates),\n",
        "            'Entities_Vector': [np.zeros(len(entity_labels))] * len(all_dates)\n",
        "        })\n",
        "\n",
        "    # Ensure the columns have correct suffixes\n",
        "    company_news_agg.rename(columns={'BERT_Embedding': 'BERT_Embedding_company', 'Sentiment': 'Sentiment_company', 'Topic': 'Topic_company', 'TFIDF': 'TFIDF_company', 'Entities_Vector': 'Entities_Vector_company'}, inplace=True)\n",
        "    all_news_agg.rename(columns={'BERT_Embedding': 'BERT_Embedding_all', 'Sentiment': 'Sentiment_all', 'Topic': 'Topic_all', 'TFIDF': 'TFIDF_all', 'Entities_Vector': 'Entities_Vector_all'}, inplace=True)\n",
        "\n",
        "    # Merge stock data with aggregated news data\n",
        "    data = pd.merge(stock_data, company_news_agg, on=\"Date\", how=\"left\")\n",
        "    data = pd.merge(data, all_news_agg, on=\"Date\", how=\"left\")\n",
        "\n",
        "    # Add fundamental data (same value for all rows as an example)\n",
        "    for key, value in fundamental_data.items():\n",
        "        data[key] = value\n",
        "\n",
        "    data[\"Company_Name\"] = company\n",
        "\n",
        "    # Add future price column\n",
        "    data[\"Future_Price\"] = data[\"Close\"].shift(-1)  # Shift price for prediction\n",
        "\n",
        "    # Drop rows where the future price is missing (typically the last row)\n",
        "    data.dropna(subset=['Future_Price'], inplace=True)\n",
        "\n",
        "    # Impute missing values in technical indicators and fundamentals\n",
        "    technical_indicator_columns = data.filter(like='ta_').columns\n",
        "    for column in technical_indicator_columns:\n",
        "        data[column].fillna(method='ffill', inplace=True)\n",
        "        data[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    fundamental_columns = [\"PE_Ratio\", \"EPS\", \"Revenue\", \"Market_Cap\"]\n",
        "    for column in fundamental_columns:\n",
        "        data[column].fillna(method='ffill', inplace=True)\n",
        "        data[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Prepare data for each company\n",
        "all_company_data = {ticker: prepare_company_data(ticker, company, from_date, to_date) for ticker, company in companies_to_focus.items()}\n",
        "\n",
        "# Check for and remove any None entries\n",
        "all_company_data = {ticker: data for ticker, data in all_company_data.items() if data is not None}\n",
        "\n",
        "if not all_company_data:\n",
        "    raise ValueError(\"No data available for any company in the specified date range.\")\n",
        "\n",
        "# Create sequences for each company\n",
        "def create_sequences(data, look_back):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - look_back):\n",
        "        sequence = {\n",
        "            \"news_embeddings_company\": np.stack(data[\"BERT_Embedding_company\"].values[i:i+look_back]),\n",
        "            \"news_embeddings_all\": np.stack(data[\"BERT_Embedding_all\"].values[i:i+look_back]),\n",
        "            \"price\": data[\"Close\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"sentiment_company\": data[\"Sentiment_company\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"sentiment_all\": data[\"Sentiment_all\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"technical_indicators\": data.filter(like='ta_').values[i:i+look_back],\n",
        "            \"fundamentals\": data[[\"PE_Ratio\", \"EPS\", \"Revenue\", \"Market_Cap\"]].values[i:i+look_back],\n",
        "            \"topic_company\": data[\"Topic_company\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"topic_all\": data[\"Topic_all\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"tfidf_company\": np.stack([np.pad(vec, (0, 1000 - len(vec))) for vec in data[\"TFIDF_company\"].values[i:i+look_back]]),\n",
        "            \"tfidf_all\": np.stack([np.pad(vec, (0, 1000 - len(vec))) for vec in data[\"TFIDF_all\"].values[i:i+look_back]]),\n",
        "            \"entities_vector_company\": np.stack(data[\"Entities_Vector_company\"].values[i:i+look_back]),\n",
        "            \"entities_vector_all\": np.stack(data[\"Entities_Vector_all\"].values[i:i+look_back])\n",
        "        }\n",
        "        sequences.append(sequence)\n",
        "        targets.append(data[\"Future_Price\"].values[i + look_back])  # Correctly assign the future price as target\n",
        "    return sequences, np.array(targets)\n",
        "\n",
        "company_sequences = {ticker: create_sequences(data, look_back) for ticker, data in all_company_data.items()}\n",
        "\n",
        "# Ensure consistency of lengths\n",
        "min_length = min(len(sequences) for sequences, _ in company_sequences.values())\n",
        "company_sequences = {ticker: (sequences[:min_length], targets[:min_length]) for ticker, (sequences, targets) in company_sequences.items()}\n",
        "\n",
        "# Convert sequences to arrays for model input\n",
        "def convert_sequences(sequences):\n",
        "    news_embeddings_company = np.array([seq[\"news_embeddings_company\"] for seq in sequences])\n",
        "    news_embeddings_all = np.array([seq[\"news_embeddings_all\"] for seq in sequences])\n",
        "    price = np.array([seq[\"price\"] for seq in sequences])\n",
        "    sentiment_company = np.array([seq[\"sentiment_company\"] for seq in sequences])\n",
        "    sentiment_all = np.array([seq[\"sentiment_all\"] for seq in sequences])\n",
        "    technical_indicators = np.array([seq[\"technical_indicators\"] for seq in sequences])\n",
        "    fundamentals = np.array([seq[\"fundamentals\"] for seq in sequences])\n",
        "    topic_company = np.array([seq[\"topic_company\"] for seq in sequences])\n",
        "    topic_all = np.array([seq[\"topic_all\"] for seq in sequences])\n",
        "    tfidf_company = np.array([seq[\"tfidf_company\"] for seq in sequences])\n",
        "    tfidf_all = np.array([seq[\"tfidf_all\"] for seq in sequences])\n",
        "    entities_vector_company = np.array([seq[\"entities_vector_company\"] for seq in sequences])\n",
        "    entities_vector_all = np.array([seq[\"entities_vector_all\"] for seq in sequences])\n",
        "    return news_embeddings_company, news_embeddings_all, price, sentiment_company, sentiment_all, technical_indicators, fundamentals, topic_company, topic_all, tfidf_company, tfidf_all, entities_vector_company, entities_vector_all\n",
        "\n",
        "company_features = {ticker: (convert_sequences(sequences), targets) for ticker, (sequences, targets) in company_sequences.items()}\n",
        "\n",
        "# Validate lengths of the features\n",
        "for key, (value, targets) in company_features.items():\n",
        "    print(f\"{key} lengths: {[len(x) for x in value]}, targets length: {len(targets)}\")\n",
        "\n",
        "# Combine all features into a single array\n",
        "def combine_features(features):\n",
        "    combined = np.concatenate([features[0],\n",
        "                               features[1],\n",
        "                               features[2],\n",
        "                               features[3],\n",
        "                               features[4],\n",
        "                               features[5],\n",
        "                               features[6],\n",
        "                               features[7],\n",
        "                               features[8],\n",
        "                               features[9],\n",
        "                               features[10],\n",
        "                               features[11],\n",
        "                               features[12]], axis=-1)\n",
        "    return combined\n",
        "\n",
        "combined_features = {ticker: combine_features(features) for ticker, (features, _) in company_features.items()}\n",
        "combined_features_array = np.concatenate(list(combined_features.values()), axis=0)\n",
        "\n",
        "# Concatenate all targets into a single array along the correct axis\n",
        "targets_array = np.concatenate([targets.reshape(-1, 1) for _, targets in company_features.values()], axis=0)\n",
        "\n",
        "# Ensure the shape of targets matches the expected dimensions\n",
        "targets_array = targets_array.reshape(-1, len(companies_to_focus))\n",
        "\n",
        "# Convert targets to a DataFrame for multi-output regression\n",
        "targets_df = pd.DataFrame(targets_array, columns=companies_to_focus.keys())\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "combined_features_array_scaled = scaler.fit_transform(combined_features_array.reshape(-1, combined_features_array.shape[-1]))\n",
        "combined_features_array_scaled = combined_features_array_scaled.reshape(combined_features_array.shape)\n",
        "\n",
        "# Scale the targets (future prices) individually for each company\n",
        "target_scalers = {ticker: StandardScaler() for ticker in companies_to_focus.keys()}\n",
        "targets_array_scaled = np.zeros_like(targets_array)\n",
        "\n",
        "for i, ticker in enumerate(companies_to_focus.keys()):\n",
        "    targets_array_scaled[:, i] = target_scalers[ticker].fit_transform(targets_array[:, i].reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert targets to a DataFrame for multi-output regression\n",
        "targets_df_scaled = pd.DataFrame(targets_array_scaled, columns=companies_to_focus.keys())\n",
        "\n",
        "# Ensure the number of samples is the same\n",
        "if combined_features_array.shape[0] != targets_df_scaled.shape[0]:\n",
        "    min_samples = min(combined_features_array.shape[0], targets_df_scaled.shape[0])\n",
        "    combined_features_array = combined_features_array[:min_samples]\n",
        "    targets_df_scaled = targets_df_scaled.iloc[:min_samples]\n",
        "\n",
        "# Prepare your data\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for train_index, val_index in tscv.split(combined_features_array):\n",
        "    X_train, X_val = combined_features_array[train_index], combined_features_array[val_index]\n",
        "    y_train, y_val = targets_df_scaled.values[train_index], targets_df_scaled.values[val_index]\n",
        "\n",
        "# Define the model\n",
        "def build_model(look_back, combined_dim, num_companies, num_heads=12, ff_dim=128, dropout_rate=0.5):\n",
        "    combined_input = tf.keras.layers.Input(shape=(look_back, combined_dim), name='combined_input')\n",
        "\n",
        "    # Register the custom layer for deserialization\n",
        "    @tf.keras.utils.register_keras_serializable()\n",
        "    # Transformer block\n",
        "    class TransformerBlock(tf.keras.layers.Layer):\n",
        "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "            super(TransformerBlock, self).__init__()\n",
        "            self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "            self.ffn = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                tf.keras.layers.Dense(embed_dim),\n",
        "            ])\n",
        "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        def call(self, inputs, training):\n",
        "            attn_output = self.att(inputs, inputs)\n",
        "            attn_output = self.dropout1(attn_output, training=training)\n",
        "            out1 = self.layernorm1(inputs + attn_output)\n",
        "            ffn_output = self.ffn(out1)\n",
        "            ffn_output = self.dropout2(ffn_output, training=training)\n",
        "            return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    transformer_block = TransformerBlock(combined_dim, num_heads, ff_dim, rate=dropout_rate)\n",
        "    x = transformer_block(combined_input)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dense layer with Batch Normalization and Dropout\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Output layers for each company\n",
        "    outputs = {ticker: tf.keras.layers.Dense(1, activation='linear', name=f'output_{ticker}')(x) for ticker in companies_to_focus.keys()}\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.models.Model(inputs=combined_input, outputs=outputs)\n",
        "\n",
        "    # Compile model with a dictionary of losses\n",
        "    losses = {ticker: 'mse' for ticker in companies_to_focus.keys()}\n",
        "    model.compile(loss=losses, optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "    return model\n",
        "\n",
        "look_back = 5  # Define the look_back as per your data\n",
        "combined_dim = combined_features_array.shape[-1]  # Combined dimension\n",
        "\n",
        "model = build_model(look_back, combined_dim, len(companies_to_focus), 12, 128, 0.5)\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 50\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, {ticker: y_train[:, i] for i, ticker in enumerate(companies_to_focus.keys())},\n",
        "          validation_data=(X_val, {ticker: y_val[:, i] for i, ticker in enumerate(companies_to_focus.keys())}),\n",
        "          epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "# Make predictions on validation data\n",
        "predicted_prices_scaled = model.predict(X_val)\n",
        "\n",
        "# Inverse transform the predictions to get the original scale\n",
        "predicted_prices = {ticker: target_scalers[ticker].inverse_transform(predictions) for ticker, predictions in predicted_prices_scaled.items()}\n",
        "\n",
        "# Display the predicted prices in the original scale\n",
        "print(predicted_prices)\n",
        "\n",
        "# Save the retrained model\n",
        "model.save('trained_model.h5')"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlimLe/ML4B-Stock-Prediction/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy spacy yfinance scikit-learn transformers tensorflow nltk ta textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M0NvyPJR11bm",
        "outputId": "261b26a5-07e0-4131-db9d-e6bf5020b2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.40)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29411 sha256=1673ce2133f872e0f96d458ff2470da6c2f10fdb7a690bb7cef918d248c526dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZnpZkIy1UPy",
        "outputId": "b148802e-6c04-4751-e782-abb549f61aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for Amazon (AMZN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for Google (GOOGL)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for Apple (AAPL)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AMZN lengths: [245, 245, 245, 245, 245, 245, 245], targets length: 245\n",
            "GOOGL lengths: [245, 245, 245, 245, 245, 245, 245], targets length: 245\n",
            "AAPL lengths: [245, 245, 245, 245, 245, 245, 245], targets length: 245\n",
            "Epoch 1/50\n",
            "7/7 [==============================] - 40s 5s/step - loss: 11.3053 - output_AAPL_loss: 3.8322 - output_AMZN_loss: 3.6145 - output_GOOGL_loss: 3.8586 - val_loss: 3.4708 - val_output_AAPL_loss: 2.8179 - val_output_AMZN_loss: 0.2709 - val_output_GOOGL_loss: 0.3821\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 33s 5s/step - loss: 8.1061 - output_AAPL_loss: 2.3903 - output_AMZN_loss: 2.6040 - output_GOOGL_loss: 3.1118 - val_loss: 4.7726 - val_output_AAPL_loss: 0.3330 - val_output_AMZN_loss: 0.3914 - val_output_GOOGL_loss: 4.0483\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 31s 4s/step - loss: 8.1290 - output_AAPL_loss: 2.5667 - output_AMZN_loss: 2.8022 - output_GOOGL_loss: 2.7601 - val_loss: 5.4607 - val_output_AAPL_loss: 0.2637 - val_output_AMZN_loss: 0.8377 - val_output_GOOGL_loss: 4.3593\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 40s 5s/step - loss: 6.8257 - output_AAPL_loss: 2.1777 - output_AMZN_loss: 2.2132 - output_GOOGL_loss: 2.4348 - val_loss: 4.2698 - val_output_AAPL_loss: 0.2274 - val_output_AMZN_loss: 1.6897 - val_output_GOOGL_loss: 2.3527\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 33s 5s/step - loss: 7.6514 - output_AAPL_loss: 2.3113 - output_AMZN_loss: 2.6543 - output_GOOGL_loss: 2.6859 - val_loss: 9.2045 - val_output_AAPL_loss: 1.2592 - val_output_AMZN_loss: 1.5796 - val_output_GOOGL_loss: 6.3657\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 35s 5s/step - loss: 6.6570 - output_AAPL_loss: 2.3352 - output_AMZN_loss: 2.2595 - output_GOOGL_loss: 2.0624 - val_loss: 15.3996 - val_output_AAPL_loss: 3.6494 - val_output_AMZN_loss: 1.1049 - val_output_GOOGL_loss: 10.6453\n",
            "2/2 [==============================] - 2s 216ms/step\n",
            "{'AMZN': array([[148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20894],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895],\n",
            "       [148.20895]], dtype=float32), 'GOOGL': array([[161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463],\n",
            "       [161.94463]], dtype=float32), 'AAPL': array([[189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27243],\n",
            "       [189.27243],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245],\n",
            "       [189.27245]], dtype=float32)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.impute import KNNImputer\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from ta import add_all_ta_features\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "# Load new financial news dataset\n",
        "news_data = pd.read_csv('/content/final_dataset.csv')\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "news_data.rename(columns={'News Article': 'News_Article', 'Date': 'Date'}, inplace=True)\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize Spacy model and NLTK components\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# List of companies to focus on\n",
        "companies_to_focus = {\n",
        "    'AMZN': 'Amazon',\n",
        "    'GOOGL': 'Google',\n",
        "    'AAPL': 'Apple'\n",
        "}\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "# Preprocess news articles\n",
        "news_data['Processed_Article'] = news_data['News_Article'].apply(preprocess_text)\n",
        "\n",
        "# Perform Sentiment Analysis\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "news_data[\"Sentiment\"] = news_data[\"Processed_Article\"].apply(get_sentiment)\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "def get_bert_embeddings(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding\n",
        "\n",
        "# Calculate BERT embeddings for all news\n",
        "news_data[\"BERT_Embedding\"] = news_data[\"Processed_Article\"].apply(lambda x: get_bert_embeddings([x], tokenizer, bert_model)[0])\n",
        "\n",
        "# Function to fetch stock prices and fundamental data for each company\n",
        "def fetch_stock_prices(ticker, start_date, end_date):\n",
        "    try:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if stock_data.shape[0] > 14:  # Ensure there are at least 15 rows of data\n",
        "            stock_data = add_all_ta_features(stock_data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")\n",
        "            # Handle missing technical indicators\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            stock_data.iloc[:, :] = imputer.fit_transform(stock_data)\n",
        "        else:\n",
        "            print(f\"Not enough data for {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Filter out rows with missing stock prices\n",
        "        stock_data.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "        # Reset index to get the date column back after filtering\n",
        "        stock_data.reset_index(inplace=True)\n",
        "\n",
        "        return stock_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def fetch_fundamental_data(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    fundamentals = stock.info\n",
        "    return {\n",
        "        \"PE_Ratio\": fundamentals.get(\"trailingPE\", np.nan),\n",
        "        \"EPS\": fundamentals.get(\"trailingEps\", np.nan),\n",
        "        \"Revenue\": fundamentals.get(\"totalRevenue\", np.nan),\n",
        "        \"Market_Cap\": fundamentals.get(\"marketCap\", np.nan)\n",
        "    }\n",
        "\n",
        "# Correct date format and optionally extend the date range\n",
        "from_date = \"2021-01-01\"\n",
        "to_date = \"2021-12-31\"  # Extended date range\n",
        "\n",
        "# Define look-back window\n",
        "look_back = 5\n",
        "\n",
        "# Function to prepare data for each company\n",
        "def prepare_company_data(ticker, company, from_date, to_date):\n",
        "    print(f\"Fetching data for {company} ({ticker})\")\n",
        "    stock_data = fetch_stock_prices(ticker, from_date, to_date)\n",
        "    if stock_data.empty:\n",
        "        print(f\"No stock data found for {company} ({ticker})\")\n",
        "        return None\n",
        "    fundamental_data = fetch_fundamental_data(ticker)\n",
        "\n",
        "    # Filter news for the company or its ticker symbol\n",
        "    company_news = news_data[news_data['News_Article'].str.contains(company, case=False) | news_data['News_Article'].str.contains(ticker, case=False)]\n",
        "\n",
        "    # Aggregate all news by day\n",
        "    all_news_agg = news_data.groupby('Date').agg({\n",
        "        'BERT_Embedding': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "        'Sentiment': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Handle missing dates for all news\n",
        "    all_dates = pd.date_range(start=from_date, end=to_date, freq='D')\n",
        "    all_news_agg = all_news_agg.set_index('Date').reindex(all_dates).reset_index()\n",
        "    all_news_agg.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "    # Insert neutral values for missing dates\n",
        "    all_news_agg['BERT_Embedding'] = all_news_agg['BERT_Embedding'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(bert_model.config.hidden_size))\n",
        "    all_news_agg['Sentiment'] = all_news_agg['Sentiment'].fillna(0.0)\n",
        "\n",
        "    # Aggregate company-specific news by day\n",
        "    if not company_news.empty:\n",
        "        company_news_agg = company_news.groupby('Date').agg({\n",
        "            'BERT_Embedding': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "            'Sentiment': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        # Handle missing dates for company-specific news\n",
        "        company_news_agg = company_news_agg.set_index('Date').reindex(all_dates).reset_index()\n",
        "        company_news_agg.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "        # Insert neutral values for missing dates\n",
        "        company_news_agg['BERT_Embedding'] = company_news_agg['BERT_Embedding'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(bert_model.config.hidden_size))\n",
        "        company_news_agg['Sentiment'] = company_news_agg['Sentiment'].fillna(0.0)\n",
        "    else:\n",
        "        # Create empty DataFrame with the same structure\n",
        "        company_news_agg = pd.DataFrame({\n",
        "            'Date': all_dates,\n",
        "            'BERT_Embedding': [np.zeros(bert_model.config.hidden_size)] * len(all_dates),\n",
        "            'Sentiment': [0.0] * len(all_dates)\n",
        "        })\n",
        "\n",
        "    # Ensure the columns have correct suffixes\n",
        "    company_news_agg.rename(columns={'BERT_Embedding': 'BERT_Embedding_company', 'Sentiment': 'Sentiment_company'}, inplace=True)\n",
        "    all_news_agg.rename(columns={'BERT_Embedding': 'BERT_Embedding_all', 'Sentiment': 'Sentiment_all'}, inplace=True)\n",
        "\n",
        "    # Merge stock data with aggregated news data\n",
        "    data = pd.merge(stock_data, company_news_agg, on=\"Date\", how=\"left\")\n",
        "    data = pd.merge(data, all_news_agg, on=\"Date\", how=\"left\")\n",
        "\n",
        "    # Add fundamental data (same value for all rows as an example)\n",
        "    for key, value in fundamental_data.items():\n",
        "        data[key] = value\n",
        "\n",
        "    data[\"Company_Name\"] = company\n",
        "\n",
        "    # Add future price column\n",
        "    data[\"Future_Price\"] = data[\"Close\"].shift(-1)  # Shift price for prediction\n",
        "\n",
        "    # Drop rows where the future price is missing (typically the last row)\n",
        "    data.dropna(subset=['Future_Price'], inplace=True)\n",
        "\n",
        "    # Impute missing values in technical indicators and fundamentals\n",
        "    technical_indicator_columns = data.filter(like='ta_').columns\n",
        "    for column in technical_indicator_columns:\n",
        "        data[column].fillna(method='ffill', inplace=True)\n",
        "        data[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    fundamental_columns = [\"PE_Ratio\", \"EPS\", \"Revenue\", \"Market_Cap\"]\n",
        "    for column in fundamental_columns:\n",
        "        data[column].fillna(method='ffill', inplace=True)\n",
        "        data[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Prepare data for each company\n",
        "all_company_data = {ticker: prepare_company_data(ticker, company, from_date, to_date) for ticker, company in companies_to_focus.items()}\n",
        "\n",
        "# Check for and remove any None entries\n",
        "all_company_data = {ticker: data for ticker, data in all_company_data.items() if data is not None}\n",
        "\n",
        "if not all_company_data:\n",
        "    raise ValueError(\"No data available for any company in the specified date range.\")\n",
        "\n",
        "# Create sequences for each company\n",
        "def create_sequences(data, look_back):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - look_back):\n",
        "        sequence = {\n",
        "            \"news_embeddings_company\": np.stack(data[\"BERT_Embedding_company\"].values[i:i+look_back]),\n",
        "            \"news_embeddings_all\": np.stack(data[\"BERT_Embedding_all\"].values[i:i+look_back]),\n",
        "            \"price\": data[\"Close\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"sentiment_company\": data[\"Sentiment_company\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"sentiment_all\": data[\"Sentiment_all\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"technical_indicators\": data.filter(like='ta_').values[i:i+look_back],\n",
        "            \"fundamentals\": data[[\"PE_Ratio\", \"EPS\", \"Revenue\", \"Market_Cap\"]].values[i:i+look_back]\n",
        "        }\n",
        "        sequences.append(sequence)\n",
        "        targets.append(data[\"Future_Price\"].values[i + look_back])  # Correctly assign the future price as target\n",
        "    return sequences, np.array(targets)\n",
        "\n",
        "company_sequences = {ticker: create_sequences(data, look_back) for ticker, data in all_company_data.items()}\n",
        "\n",
        "# Ensure consistency of lengths\n",
        "min_length = min(len(sequences) for sequences, _ in company_sequences.values())\n",
        "company_sequences = {ticker: (sequences[:min_length], targets[:min_length]) for ticker, (sequences, targets) in company_sequences.items()}\n",
        "\n",
        "# Convert sequences to arrays for model input\n",
        "def convert_sequences(sequences):\n",
        "    news_embeddings_company = np.array([seq[\"news_embeddings_company\"] for seq in sequences])\n",
        "    news_embeddings_all = np.array([seq[\"news_embeddings_all\"] for seq in sequences])\n",
        "    price = np.array([seq[\"price\"] for seq in sequences])\n",
        "    sentiment_company = np.array([seq[\"sentiment_company\"] for seq in sequences])\n",
        "    sentiment_all = np.array([seq[\"sentiment_all\"] for seq in sequences])\n",
        "    technical_indicators = np.array([seq[\"technical_indicators\"] for seq in sequences])\n",
        "    fundamentals = np.array([seq[\"fundamentals\"] for seq in sequences])\n",
        "    return news_embeddings_company, news_embeddings_all, price, sentiment_company, sentiment_all, technical_indicators, fundamentals\n",
        "\n",
        "company_features = {ticker: (convert_sequences(sequences), targets) for ticker, (sequences, targets) in company_sequences.items()}\n",
        "\n",
        "# Validate lengths of the features\n",
        "for key, (value, targets) in company_features.items():\n",
        "    print(f\"{key} lengths: {[len(x) for x in value]}, targets length: {len(targets)}\")\n",
        "# Combine all features into a single array\n",
        "def combine_features(features):\n",
        "    combined = np.concatenate([features[0],\n",
        "                               features[1],\n",
        "                               features[2],\n",
        "                               features[3],\n",
        "                               features[4],\n",
        "                               features[5],\n",
        "                               features[6]], axis=-1)\n",
        "    return combined\n",
        "\n",
        "combined_features = {ticker: combine_features(features) for ticker, (features, _) in company_features.items()}\n",
        "combined_features_array = np.concatenate(list(combined_features.values()), axis=0)\n",
        "\n",
        "# Concatenate all targets into a single array along the correct axis\n",
        "targets_array = np.concatenate([targets.reshape(-1, 1) for _, targets in company_features.values()], axis=0)\n",
        "\n",
        "# Ensure the shape of targets matches the expected dimensions\n",
        "targets_array = targets_array.reshape(-1, len(companies_to_focus))\n",
        "\n",
        "# Convert targets to a DataFrame for multi-output regression\n",
        "targets_df = pd.DataFrame(targets_array, columns=companies_to_focus.keys())\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "combined_features_array_scaled = scaler.fit_transform(combined_features_array.reshape(-1, combined_features_array.shape[-1]))\n",
        "combined_features_array_scaled = combined_features_array_scaled.reshape(combined_features_array.shape)\n",
        "\n",
        "# Scale the targets (future prices) individually for each company\n",
        "target_scalers = {ticker: StandardScaler() for ticker in companies_to_focus.keys()}\n",
        "targets_array_scaled = np.zeros_like(targets_array)\n",
        "\n",
        "for i, ticker in enumerate(companies_to_focus.keys()):\n",
        "    targets_array_scaled[:, i] = target_scalers[ticker].fit_transform(targets_array[:, i].reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert targets to a DataFrame for multi-output regression\n",
        "targets_df_scaled = pd.DataFrame(targets_array_scaled, columns=companies_to_focus.keys())\n",
        "\n",
        "# Ensure the number of samples is the same\n",
        "if combined_features_array.shape[0] != targets_df_scaled.shape[0]:\n",
        "    min_samples = min(combined_features_array.shape[0], targets_df_scaled.shape[0])\n",
        "    combined_features_array = combined_features_array[:min_samples]\n",
        "    targets_df_scaled = targets_df_scaled.iloc[:min_samples]\n",
        "\n",
        "# Prepare data\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for train_index, val_index in tscv.split(combined_features_array):\n",
        "    X_train, X_val = combined_features_array[train_index], combined_features_array[val_index]\n",
        "    y_train, y_val = targets_df_scaled.values[train_index], targets_df_scaled.values[val_index]\n",
        "\n",
        "# Define the model\n",
        "def build_model(look_back, combined_dim, num_companies, num_heads=12, ff_dim=128, dropout_rate=0.5):\n",
        "    combined_input = tf.keras.layers.Input(shape=(look_back, combined_dim), name='combined_input')\n",
        "\n",
        "    # Register the custom layer for deserialization\n",
        "    @tf.keras.utils.register_keras_serializable()\n",
        "    # Transformer block\n",
        "    class TransformerBlock(tf.keras.layers.Layer):\n",
        "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "            super(TransformerBlock, self).__init__()\n",
        "            self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "            self.ffn = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                tf.keras.layers.Dense(embed_dim),\n",
        "            ])\n",
        "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        def call(self, inputs, training):\n",
        "            attn_output = self.att(inputs, inputs)\n",
        "            attn_output = self.dropout1(attn_output, training=training)\n",
        "            out1 = self.layernorm1(inputs + attn_output)\n",
        "            ffn_output = self.ffn(out1)\n",
        "            ffn_output = self.dropout2(ffn_output, training=training)\n",
        "            return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    transformer_block = TransformerBlock(combined_dim, num_heads, ff_dim, rate=dropout_rate)\n",
        "    x = transformer_block(combined_input)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dense layer with Batch Normalization and Dropout\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Output layers for each company\n",
        "    outputs = {ticker: tf.keras.layers.Dense(1, activation='linear', name=f'output_{ticker}')(x) for ticker in companies_to_focus.keys()}\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.models.Model(inputs=combined_input, outputs=outputs)\n",
        "\n",
        "    # Compile model with a dictionary of losses\n",
        "    losses = {ticker: 'mse' for ticker in companies_to_focus.keys()}\n",
        "    model.compile(loss=losses, optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "    return model\n",
        "\n",
        "look_back = 5  # Define the look_back as per data\n",
        "combined_dim = combined_features_array.shape[-1]  # Combined dimension\n",
        "\n",
        "model = build_model(look_back, combined_dim, len(companies_to_focus), 12, 128, 0.5)\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 50\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "# Make predictions on validation data\n",
        "predicted_prices_scaled = model.predict(X_val)\n",
        "\n",
        "# Inverse transform the predictions to get the original scale\n",
        "predicted_prices = {ticker: target_scalers[ticker].inverse_transform(predictions) for ticker, predictions in predicted_prices_scaled.items()}\n",
        "\n",
        "# Display the predicted prices in the original scale\n",
        "print(predicted_prices)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoyTWwlbAi-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('trained_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CQwuZvx9i7c",
        "outputId": "a037f0b9-4743-427e-f7dc-49d50b066d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import plotly.graph_objs as go\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gdown\n",
        "\n",
        "# Define the company tickers and names\n",
        "companies_to_focus = {\n",
        "    'AMZN': 'Amazon',\n",
        "    'GOOGL': 'Google',\n",
        "    'AAPL': 'Apple'\n",
        "}\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Define lookback window\n",
        "look_back = 5\n",
        "\n",
        "# Register the custom layer for deserialization\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Load the trained model with the custom layer\n",
        "custom_objects = {'TransformerBlock': TransformerBlock}\n",
        "model = tf.keras.models.load_model('/content/trained_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "# Function to preprocess text for BERT embeddings\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    text = text.lower().strip()\n",
        "    tokens = text.split()\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embeddings(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding\n",
        "\n",
        "# Function to predict future prices\n",
        "def predict_prices(news_headlines, look_back_window, bert_dim, combined_dim, scaler, target_scalers):\n",
        "    processed_articles = [preprocess_text(article) for article in news_headlines]\n",
        "    bert_embeddings = [get_bert_embeddings([article], tokenizer, bert_model)[0] for article in processed_articles]\n",
        "\n",
        "    # Ensure the embeddings have the correct shape\n",
        "    bert_embeddings = bert_embeddings[-look_back_window:]\n",
        "    if len(bert_embeddings) < look_back_window:\n",
        "        # Pad the embeddings if there are not enough look-back days\n",
        "        padding = [np.zeros((bert_dim,)) for _ in range(look_back_window - len(bert_embeddings))]\n",
        "        bert_embeddings = padding + bert_embeddings\n",
        "\n",
        "    if combined_dim > bert_dim:\n",
        "        # Combine with dummy data to match the expected combined dimension\n",
        "        dummy_data = np.zeros((look_back_window, combined_dim - bert_dim))\n",
        "        combined_features = np.concatenate([bert_embeddings, dummy_data], axis=-1)\n",
        "    else:\n",
        "        combined_features = np.array(bert_embeddings)\n",
        "\n",
        "    # Reshape for model input\n",
        "    combined_features = np.array(combined_features).reshape(1, look_back_window, -1)\n",
        "\n",
        "    # Scale the combined features\n",
        "    combined_features_scaled = scaler.transform(combined_features.reshape(-1, combined_features.shape[-1]))\n",
        "    combined_features_scaled = combined_features_scaled.reshape(combined_features.shape)\n",
        "\n",
        "    # Predict using the loaded model\n",
        "    predictions_scaled = model.predict(combined_features_scaled)\n",
        "\n",
        "    # Inverse transform the predictions to get the original scale\n",
        "    predictions = {ticker: target_scalers[ticker].inverse_transform(predictions_scaled[ticker]) for ticker in companies_to_focus.keys()}\n",
        "    return predictions\n",
        "\n",
        "# Function to perform sentiment analysis\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Function to fetch fundamental data for a company\n",
        "def fetch_fundamental_data(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    fundamentals = stock.info\n",
        "    return {\n",
        "        \"PE_Ratio\": fundamentals.get(\"trailingPE\", np.nan),\n",
        "        \"EPS\": fundamentals.get(\"trailingEps\", np.nan),\n",
        "        \"Revenue\": fundamentals.get(\"totalRevenue\", np.nan),\n",
        "        \"Market_Cap\": fundamentals.get(\"marketCap\", np.nan)\n",
        "    }\n",
        "\n",
        "# Load the dataset\n",
        "news_data = pd.read_csv('/content/final_dataset_without_last_column.csv')\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "news_data['Processed_Article'] = news_data['News_Article'].apply(preprocess_text)\n",
        "news_data['Sentiment'] = news_data['Processed_Article'].apply(get_sentiment)\n",
        "\n",
        "# Define dimensions\n",
        "bert_dim = bert_model.config.hidden_size  # typically 768 for BERT models\n",
        "combined_dim = 1543  # Update this to the correct combined dimension\n",
        "\n",
        "# Initialize scalers\n",
        "scaler = StandardScaler()\n",
        "target_scalers = {ticker: StandardScaler() for ticker in companies_to_focus.keys()}\n",
        "\n",
        "# Simulate fitting scalers with initial data\n",
        "def fit_scalers():\n",
        "    combined_features_list = []\n",
        "    targets_list = []\n",
        "\n",
        "    for ticker in companies_to_focus.keys():\n",
        "        # Simulate fetching stock data\n",
        "        stock_data = yf.download(ticker, start='2021-01-01', end='2021-12-31')\n",
        "        stock_data.reset_index(inplace=True)\n",
        "\n",
        "        # Fetch moving averages\n",
        "        ma50 = stock_data['Close'].rolling(window=50).mean()\n",
        "        ma200 = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "        stock_data['MA50'] = ma50\n",
        "        stock_data['MA200'] = ma200\n",
        "\n",
        "        # Generate dummy combined features matching the expected combined dimension\n",
        "        num_samples = len(stock_data)\n",
        "        dummy_bert_features = np.zeros((num_samples, 768))  # Example BERT feature size\n",
        "        dummy_other_features = np.zeros((num_samples, combined_dim - 768))\n",
        "        combined_features = np.hstack([dummy_bert_features, dummy_other_features])\n",
        "\n",
        "        combined_features_list.append(combined_features)\n",
        "        targets_list.append(stock_data['Close'].values)\n",
        "\n",
        "    combined_features_array = np.concatenate(combined_features_list, axis=0)\n",
        "    targets_array = np.concatenate(targets_list, axis=0).reshape(-1, len(companies_to_focus))\n",
        "\n",
        "    scaler.fit(combined_features_array)\n",
        "    for i, ticker in enumerate(companies_to_focus.keys()):\n",
        "        target_scalers[ticker].fit(targets_array[:, i].reshape(-1, 1))\n",
        "\n",
        "fit_scalers()\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"Stock Price Prediction App\")\n",
        "\n",
        "# Sidebar Description\n",
        "st.sidebar.title(\"About the App\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "This application predicts the stock prices of major companies using news headlines and sentiment analysis.\n",
        "We utilize BERT embeddings, technical indicators, and fundamental data for robust predictions.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.title(\"Model Description\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "Our model leverages a transformer-based architecture with BERT embeddings to capture the semantic meaning of news articles.\n",
        "We incorporate technical indicators, such as moving averages, and fundamental data to improve the prediction accuracy.\n",
        "\"\"\")\n",
        "\n",
        "# Fetch data\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "start_date = (datetime.today() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "end_date = today\n",
        "\n",
        "# Get today's news headlines\n",
        "todays_news = news_data[news_data['Date'] == today].head(6)  # Display at most 6 headlines\n",
        "\n",
        "# Get stock data and predictions\n",
        "stock_data_dict = {}\n",
        "fundamental_data_dict = {}\n",
        "for ticker in companies_to_focus:\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "    # Ensure the Date column is present\n",
        "    stock_data.reset_index(inplace=True)\n",
        "\n",
        "    # Fetch moving averages\n",
        "    ma50 = stock_data['Close'].rolling(window=50).mean()\n",
        "    ma200 = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "    stock_data['MA50'] = ma50\n",
        "    stock_data['MA200'] = ma200\n",
        "\n",
        "    stock_data_dict[ticker] = stock_data\n",
        "    fundamental_data_dict[ticker] = fetch_fundamental_data(ticker)\n",
        "\n",
        "# Call predict_prices once\n",
        "news_headlines = todays_news['Processed_Article'].tolist()\n",
        "predictions = predict_prices(news_headlines, look_back, bert_dim, combined_dim, scaler, target_scalers)\n",
        "predictions_dict = {ticker: predictions[ticker] for ticker in companies_to_focus}\n",
        "\n",
        "# Display predicted prices for tomorrow\n",
        "st.subheader(\"Predicted Prices for Tomorrow\")\n",
        "for ticker, company in companies_to_focus.items():\n",
        "    today_price = stock_data_dict[ticker]['Close'].values[-1]\n",
        "    predicted_price = predictions_dict[ticker][0][0]\n",
        "    arrow = \"\" if predicted_price > today_price else \"\"\n",
        "    color = \"green\" if predicted_price > today_price else \"red\"\n",
        "    st.markdown(f\"**{company} ({ticker}):** {predicted_price:.2f} {arrow}\", unsafe_allow_html=True)\n",
        "\n",
        "# Display news headlines with sentiment in a table\n",
        "st.subheader(\"Latest News\")\n",
        "news_table = todays_news[['News_Article', 'Sentiment']].copy()\n",
        "news_table.columns = ['News Article', 'Sentiment']\n",
        "news_table['Sentiment'] = news_table['Sentiment'].apply(lambda x: f\"<span style='color:{'green' if x > 0 else 'red'}'>{x:.2f}</span>\")\n",
        "st.write(news_table.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
        "\n",
        "# Manual prediction input\n",
        "st.subheader(\"Manual Prediction Input\")\n",
        "manual_news_headlines = st.text_area(\"Enter News Headlines\", \"\").split('\\n')\n",
        "\n",
        "if st.button(\"Predict Manually\"):\n",
        "    if manual_news_headlines:\n",
        "        manual_predictions = predict_prices(manual_news_headlines, look_back, bert_dim, combined_dim, scaler, target_scalers)\n",
        "        for ticker, company in companies_to_focus.items():\n",
        "            manual_prediction = manual_predictions[ticker][0][0]\n",
        "            today_price = stock_data_dict[ticker]['Close'].values[-1]\n",
        "            arrow = \"\" if manual_prediction > today_price else \"\"\n",
        "            st.write(f\"Predicted price for {company} ({ticker}): {manual_prediction:.2f} {arrow}\")\n",
        "\n",
        "# Display stock price charts with actual, predicted prices, and technical indicators\n",
        "for ticker, company in companies_to_focus.items():\n",
        "    stock_data = stock_data_dict[ticker]\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add actual stock price trace\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['Close'], mode='lines', name='Actual Close'))\n",
        "\n",
        "    # Add predicted price trace\n",
        "    predicted_price = predictions_dict[ticker][0][0]\n",
        "    predicted_date = stock_data['Date'].iloc[-1] + timedelta(days=1)\n",
        "    fig.add_trace(go.Scatter(x=[predicted_date], y=[predicted_price], mode='markers', name='Predicted Close', marker=dict(color='red', size=10)))\n",
        "\n",
        "    # Add moving average traces\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['MA50'], mode='lines', name='MA50'))\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['MA200'], mode='lines', name='MA200'))\n",
        "\n",
        "    # Customize the layout\n",
        "    fig.update_layout(\n",
        "        title=f'{company} ({ticker}) Stock Prices',\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Price',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Display the chart\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Display fundamental data\n",
        "    st.subheader(f\"{company} ({ticker}) Fundamentals\")\n",
        "    fundamentals = fundamental_data_dict[ticker]\n",
        "    st.markdown(f\"\"\"\n",
        "    - **PE Ratio**: {fundamentals['PE_Ratio']}\n",
        "    - **EPS**: {fundamentals['EPS']}\n",
        "    - **Revenue**: {fundamentals['Revenue']}\n",
        "    - **Market Cap**: {fundamentals['Market_Cap']}\n",
        "    \"\"\")\n",
        "\n",
        "# \"See More\" Section\n",
        "st.subheader(\"See More\")\n",
        "st.markdown(\"\"\"\n",
        "We also trained a model that uses Topic Modelling, TF-IDF, and Named Entity Recognition (NER) as features.\n",
        "For more details, check out our [GitHub Repository](https://github.com/your-repo-link).\n",
        "\"\"\")\n",
        "\n",
        "# End of the Streamlit app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrS4RVQTD7-h",
        "outputId": "69d09287-399e-4e92-b892-b65fdf0a80c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "25dip5eq-iIs",
        "outputId": "041f8bca-9696-47c3-8907-0dd429b2b9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize ngrok\n",
        "ngrok.set_auth_token('2h6sfydzZ90UINBPKt8DX3rmu1h_6jzyuJACPKAhjFUh64RAx')  # Get your auth token from ngrok\n",
        "\n",
        "# Kill any previous tunnels if open\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "# Display the public URL\n",
        "print(f'Streamlit App is available at: {public_url}')\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf_un8wfBG9n",
        "outputId": "7a236176-66af-4819-c2b5-5aa949ed767f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App is available at: NgrokTunnel: \"https://4019-34-139-105-70.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8F+o/sjuNZGtwo2o5CJUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlimLe/ML4B-Stock-Prediction/blob/main/Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab file contains two cells.\n",
        "\n",
        "The first cell creates a streamlit app using the %%writefile app.py command.\n",
        "\n",
        "To deploy the app, follow these steps:\n",
        "*   Run the first cell to write the Streamlit app code to app.py\n",
        "*   Run the second cell to start the deployment process. This cell ensures  that all tunnels are properly set up. After running this cell, click the HTTPS link that appears to access your Streamlit app\n",
        "\n",
        "\n",
        "If you encounter any issues, such as connection errors or other types of errors, follow these steps:\n",
        "\n",
        "*   Rerun the second cell. This will kill any existing tunnels and attempt to re-establish a new connection.\n",
        "*   If the problem persists, rerun the second cell again until the connection stabilizes and the app runs smoothly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RzciIRCm_xKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import plotly.graph_objs as go\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gdown\n",
        "\n",
        "# Download the model from Google Drive\n",
        "url = 'https://drive.google.com/uc?id=1jb-sKw4SS4NjLyIdVt9m9E008mLVBzJK'\n",
        "output = 'basic_model.h5'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Download the dataset from Google Drive\n",
        "dataset_url = 'https://drive.google.com/uc?id=11wSYecYRu9bCwfT3PeQwcHFVIr9AEWdy'\n",
        "dataset_output = 'final_dataset_without_last_column.csv'\n",
        "gdown.download(dataset_url, dataset_output, quiet=False)\n",
        "\n",
        "# Define the company tickers and names\n",
        "companies_to_focus = {\n",
        "    'AMZN': 'Amazon',\n",
        "    'GOOGL': 'Google',\n",
        "    'AAPL': 'Apple'\n",
        "}\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Define lookback window\n",
        "look_back = 5\n",
        "\n",
        "# Register the custom layer for deserialization\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Load the trained model with the custom layer\n",
        "custom_objects = {'TransformerBlock': TransformerBlock}\n",
        "model = tf.keras.models.load_model('basic_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "# Function to preprocess text for BERT embeddings\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    text = text.lower().strip()\n",
        "    tokens = text.split()\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embeddings(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding\n",
        "\n",
        "# Function to predict future prices\n",
        "def predict_prices(news_headlines, look_back_window, bert_dim, combined_dim, scaler, target_scalers):\n",
        "    processed_articles = [preprocess_text(article) for article in news_headlines]\n",
        "    bert_embeddings = [get_bert_embeddings([article], tokenizer, bert_model)[0] for article in processed_articles]\n",
        "\n",
        "    # Ensure the embeddings have the correct shape\n",
        "    bert_embeddings = bert_embeddings[-look_back_window:]\n",
        "    if len(bert_embeddings) < look_back_window:\n",
        "        # Pad the embeddings if there are not enough look-back days\n",
        "        padding = [np.zeros((bert_dim,)) for _ in range(look_back_window - len(bert_embeddings))]\n",
        "        bert_embeddings = padding + bert_embeddings\n",
        "\n",
        "    if combined_dim > bert_dim:\n",
        "        # Combine with dummy data to match the expected combined dimension\n",
        "        dummy_data = np.zeros((look_back_window, combined_dim - bert_dim))\n",
        "        combined_features = np.concatenate([bert_embeddings, dummy_data], axis=-1)\n",
        "    else:\n",
        "        combined_features = np.array(bert_embeddings)\n",
        "\n",
        "    # Reshape for model input\n",
        "    combined_features = np.array(combined_features).reshape(1, look_back_window, -1)\n",
        "\n",
        "    # Scale the combined features\n",
        "    combined_features_scaled = scaler.transform(combined_features.reshape(-1, combined_features.shape[-1]))\n",
        "    combined_features_scaled = combined_features_scaled.reshape(combined_features.shape)\n",
        "\n",
        "    # Predict using the loaded model\n",
        "    predictions_scaled = model.predict(combined_features_scaled)\n",
        "\n",
        "    # Inverse transform the predictions to get the original scale\n",
        "    predictions = {ticker: target_scalers[ticker].inverse_transform(predictions_scaled[ticker]) for ticker in companies_to_focus.keys()}\n",
        "    return predictions\n",
        "\n",
        "# Function to perform sentiment analysis\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Function to fetch fundamental data for a company\n",
        "def fetch_fundamental_data(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    fundamentals = stock.info\n",
        "    return {\n",
        "        \"PE_Ratio\": fundamentals.get(\"trailingPE\", np.nan),\n",
        "        \"EPS\": fundamentals.get(\"trailingEps\", np.nan),\n",
        "        \"Revenue\": fundamentals.get(\"totalRevenue\", np.nan),\n",
        "        \"Market_Cap\": fundamentals.get(\"marketCap\", np.nan)\n",
        "    }\n",
        "\n",
        "# Load the dataset\n",
        "news_data = pd.read_csv('final_dataset_without_last_column.csv')\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "news_data['Processed_Article'] = news_data['News Article'].apply(preprocess_text)\n",
        "news_data['Sentiment'] = news_data['Processed_Article'].apply(get_sentiment)\n",
        "\n",
        "# Define dimensions\n",
        "bert_dim = bert_model.config.hidden_size  # typically 768 for BERT models\n",
        "combined_dim = 1543  # Update this to the correct combined dimension\n",
        "\n",
        "# Initialize scalers\n",
        "scaler = StandardScaler()\n",
        "target_scalers = {ticker: StandardScaler() for ticker in companies_to_focus.keys()}\n",
        "\n",
        "# Simulate fitting scalers with initial data\n",
        "def fit_scalers():\n",
        "    combined_features_list = []\n",
        "    targets_list = []\n",
        "\n",
        "    for ticker in companies_to_focus.keys():\n",
        "        # Simulate fetching stock data\n",
        "        stock_data = yf.download(ticker, start='2021-01-01', end='2021-12-31')\n",
        "        stock_data.reset_index(inplace=True)\n",
        "\n",
        "        # Fetch moving averages\n",
        "        ma50 = stock_data['Close'].rolling(window=50).mean()\n",
        "        ma200 = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "        stock_data['MA50'] = ma50\n",
        "        stock_data['MA200'] = ma200\n",
        "\n",
        "        # Generate dummy combined features matching the expected combined dimension\n",
        "        num_samples = len(stock_data)\n",
        "        dummy_bert_features = np.zeros((num_samples, 768))  # Example BERT feature size\n",
        "        dummy_other_features = np.zeros((num_samples, combined_dim - 768))\n",
        "        combined_features = np.hstack([dummy_bert_features, dummy_other_features])\n",
        "\n",
        "        combined_features_list.append(combined_features)\n",
        "        targets_list.append(stock_data['Close'].values)\n",
        "\n",
        "    combined_features_array = np.concatenate(combined_features_list, axis=0)\n",
        "    targets_array = np.concatenate(targets_list, axis=0).reshape(-1, len(companies_to_focus))\n",
        "\n",
        "    scaler.fit(combined_features_array)\n",
        "    for i, ticker in enumerate(companies_to_focus.keys()):\n",
        "        target_scalers[ticker].fit(targets_array[:, i].reshape(-1, 1))\n",
        "\n",
        "fit_scalers()\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"Stock Price Prediction App\")\n",
        "\n",
        "# Sidebar Description\n",
        "st.sidebar.title(\"About the App\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "This application predicts the stock prices of major companies using news headlines and sentiment analysis.\n",
        "We utilize BERT embeddings, technical indicators, and fundamental data for robust predictions.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.title(\"Model Description\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "Our model leverages a transformer-based architecture with BERT embeddings to capture the semantic meaning of news articles.\n",
        "We incorporate technical indicators, such as moving averages, and fundamental data to improve the prediction accuracy.\n",
        "\"\"\")\n",
        "\n",
        "# Fetch data\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "start_date = (datetime.today() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "end_date = today\n",
        "\n",
        "# Get today's news headlines\n",
        "todays_news = news_data[news_data['Date'] == today].head(6)  # Display at most 6 headlines\n",
        "\n",
        "# Get stock data and predictions\n",
        "stock_data_dict = {}\n",
        "fundamental_data_dict = {}\n",
        "for ticker in companies_to_focus:\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "    # Ensure the Date column is present\n",
        "    stock_data.reset_index(inplace=True)\n",
        "\n",
        "    # Fetch moving averages\n",
        "    ma50 = stock_data['Close'].rolling(window=50).mean()\n",
        "    ma200 = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "    stock_data['MA50'] = ma50\n",
        "    stock_data['MA200'] = ma200\n",
        "\n",
        "    stock_data_dict[ticker] = stock_data\n",
        "    fundamental_data_dict[ticker] = fetch_fundamental_data(ticker)\n",
        "\n",
        "# Call predict_prices once\n",
        "news_headlines = todays_news['Processed_Article'].tolist()\n",
        "predictions = predict_prices(news_headlines, look_back, bert_dim, combined_dim, scaler, target_scalers)\n",
        "predictions_dict = {ticker: predictions[ticker] for ticker in companies_to_focus}\n",
        "\n",
        "# Display predicted prices for tomorrow\n",
        "st.subheader(\"Predicted Prices for Tomorrow\")\n",
        "for ticker, company in companies_to_focus.items():\n",
        "    today_price = stock_data_dict[ticker]['Close'].values[-1]\n",
        "    predicted_price = predictions_dict[ticker][0][0]\n",
        "    arrow = \"⬆️\" if predicted_price > today_price else \"⬇️\"\n",
        "    color = \"green\" if predicted_price > today_price else \"red\"\n",
        "    st.markdown(f\"**{company} ({ticker}):** {predicted_price:.2f} {arrow}\", unsafe_allow_html=True)\n",
        "\n",
        "# Display news headlines with sentiment in a table\n",
        "st.subheader(\"Latest News\")\n",
        "news_table = todays_news[['News Article', 'Sentiment']].copy()\n",
        "news_table['Sentiment'] = news_table['Sentiment'].apply(lambda x: f\"<span style='color:{'green' if x > 0 else 'red'}'>{x:.2f}</span>\")\n",
        "st.write(news_table.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
        "\n",
        "# Manual prediction input\n",
        "st.subheader(\"Manual Prediction Input\")\n",
        "manual_news_headlines = st.text_area(\"Enter News Headlines\", \"\").split('\\n')\n",
        "\n",
        "if st.button(\"Predict Manually\"):\n",
        "    if manual_news_headlines:\n",
        "        manual_predictions = predict_prices(manual_news_headlines, look_back, bert_dim, combined_dim, scaler, target_scalers)\n",
        "        for ticker, company in companies_to_focus.items():\n",
        "            manual_prediction = manual_predictions[ticker][0][0]\n",
        "            today_price = stock_data_dict[ticker]['Close'].values[-1]\n",
        "            arrow = \"⬆️\" if manual_prediction > today_price else \"⬇️\"\n",
        "            st.write(f\"Predicted price for {company} ({ticker}): {manual_prediction:.2f} {arrow}\")\n",
        "\n",
        "# Display stock price charts with actual, predicted prices, and technical indicators\n",
        "for ticker, company in companies_to_focus.items():\n",
        "    stock_data = stock_data_dict[ticker]\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add actual stock price trace\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['Close'], mode='lines', name='Actual Close'))\n",
        "\n",
        "    # Add predicted price trace\n",
        "    predicted_price = predictions_dict[ticker][0][0]\n",
        "    predicted_date = stock_data['Date'].iloc[-1] + timedelta(days=1)\n",
        "    fig.add_trace(go.Scatter(x=[predicted_date], y=[predicted_price], mode='markers', name='Predicted Close', marker=dict(color='red', size=10)))\n",
        "\n",
        "    # Add moving average traces\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['MA50'], mode='lines', name='MA50'))\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['MA200'], mode='lines', name='MA200'))\n",
        "\n",
        "    # Customize the layout\n",
        "    fig.update_layout(\n",
        "        title=f'{company} ({ticker}) Stock Prices',\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Price',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Display the chart\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Display fundamental data\n",
        "    st.subheader(f\"{company} ({ticker}) Fundamentals\")\n",
        "    fundamentals = fundamental_data_dict[ticker]\n",
        "    st.markdown(f\"\"\"\n",
        "    - **PE Ratio**: {fundamentals['PE_Ratio']}\n",
        "    - **EPS**: {fundamentals['EPS']}\n",
        "    - **Revenue**: {fundamentals['Revenue']}\n",
        "    - **Market Cap**: {fundamentals['Market_Cap']}\n",
        "    \"\"\")\n",
        "\n",
        "# \"See More\" Section\n",
        "st.subheader(\"See More\")\n",
        "st.markdown(\"\"\"\n",
        "We also trained a model that uses Topic Modelling, TF-IDF, and Named Entity Recognition (NER) as features.\n",
        "For more details, check out our [GitHub Repository](https://github.com/KlimLe/ML4B-Stock-Prediction/tree/main).\n",
        "\"\"\")\n",
        "\n",
        "# End of the Streamlit app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emBdp74hp2fH",
        "outputId": "3264e030-f46c-4892-8a76-cf4e04a17be5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize ngrok\n",
        "ngrok.set_auth_token('2h6sfydzZ90UINBPKt8DX3rmu1h_6jzyuJACPKAhjFUh64RAx')  # Get your auth token from ngrok\n",
        "\n",
        "# Kill any previous tunnels if open\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "# Display the public URL\n",
        "print(f'Streamlit App is available at: {public_url}')\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeLI_jlZg13l",
        "outputId": "2d041817-b2ff-49af-e4b9-2fbcc8774df8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Streamlit App is available at: NgrokTunnel: \"https://60f3-35-247-116-195.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}